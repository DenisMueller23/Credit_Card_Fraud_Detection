{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6e5b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (590540, 410)\n",
      "Test data shape: (506691, 409)\n",
      "Index(['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt',\n",
      "       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n",
      "       ...\n",
      "       'TransactionDT_hour_sin', 'TransactionDT_hour_cos',\n",
      "       'TransactionDT_dow_sin', 'TransactionDT_dow_cos',\n",
      "       'TransactionDT_month_sin', 'TransactionDT_month_cos',\n",
      "       'TransactionDT_is_weekend', 'TransactionDT_time_of_day',\n",
      "       'TransactionDT_days_since_first', 'TransactionDT_seconds_since_first'],\n",
      "      dtype='object', length=410)\n",
      "Train data shape after merge: (593424, 450)\n",
      "Test data shape after merge: (506691, 449)\n",
      "Fraud rate in training data: 0.0348\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Add these imports to your existing cell\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the cleaned data\n",
    "train_transaction = pd.read_csv('../data/preprocessed/train_transaction_cleaned.csv')\n",
    "train_identity = pd.read_csv('../data/preprocessed/train_identity_cleaned.csv')\n",
    "test_transaction = pd.read_csv('../data/preprocessed/test_transaction_cleaned.csv')\n",
    "test_identity = pd.read_csv('../data/preprocessed/test_identity_cleaned.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_transaction.shape}\")\n",
    "print(f\"Test data shape: {test_transaction.shape}\")\n",
    "print(train_transaction.columns)\n",
    "\n",
    "# Combine transaction and identity data\n",
    "train_data = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "test_data = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n",
    "print(f\"Train data shape after merge: {train_data.shape}\")\n",
    "print(f\"Test data shape after merge: {test_data.shape}\")\n",
    "print(f\"Fraud rate in training data: {train_data['isFraud'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Enhanced Temporal Features (Fix your existing function)\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Advanced temporal feature engineering with proper velocity calculation\"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df['TransactionDT_hour'] = (df['TransactionDT'] / 3600) % 24\n",
    "    df['TransactionDT_day'] = (df['TransactionDT'] / (3600 * 24)) % 7\n",
    "    df['TransactionDT_week'] = df['TransactionDT'] / (3600 * 24 * 7)\n",
    "    \n",
    "    # Advanced temporal patterns\n",
    "    df['is_weekend'] = df['TransactionDT_day'].isin([5, 6]).astype(int)\n",
    "    df['is_night'] = ((df['TransactionDT_hour'] >= 22) | (df['TransactionDT_hour'] <= 6)).astype(int)\n",
    "    df['is_business_hours'] = ((df['TransactionDT_hour'] >= 9) & (df['TransactionDT_hour'] <= 17)).astype(int)\n",
    "    df['is_peak_hours'] = df['TransactionDT_hour'].isin([12, 13, 18, 19, 20]).astype(int)\n",
    "    \n",
    "    # Time since epoch features\n",
    "    df['days_since_start'] = df['TransactionDT'] / (3600 * 24)\n",
    "    df['hour_of_week'] = df['TransactionDT_day'] * 24 + df['TransactionDT_hour']\n",
    "    \n",
    "    # Sort by card and time for velocity calculations\n",
    "    df = df.sort_values(['card1', 'TransactionDT']).reset_index(drop=True)\n",
    "    \n",
    "    # Fixed velocity features - count transactions in time windows\n",
    "    for hours in [1, 24, 168]:  # 1 hour, 1 day, 1 week\n",
    "        window_seconds = hours * 3600\n",
    "        \n",
    "        # Create velocity feature using a simpler approach\n",
    "        velocity_list = []\n",
    "        for card in df['card1'].unique():\n",
    "            card_data = df[df['card1'] == card].copy()\n",
    "            card_data = card_data.sort_values('TransactionDT')\n",
    "            \n",
    "            # Calculate velocity for each transaction\n",
    "            velocities = []\n",
    "            for i, row in card_data.iterrows():\n",
    "                current_time = row['TransactionDT']\n",
    "                time_window_start = current_time - window_seconds\n",
    "                \n",
    "                # Count transactions in the time window (including current)\n",
    "                count = len(card_data[\n",
    "                    (card_data['TransactionDT'] <= current_time) & \n",
    "                    (card_data['TransactionDT'] > time_window_start)\n",
    "                ])\n",
    "                velocities.append(count)\n",
    "            \n",
    "            # Store velocities with original indices\n",
    "            for idx, vel in zip(card_data.index, velocities):\n",
    "                velocity_list.append((idx, vel))\n",
    "        \n",
    "        # Create velocity column\n",
    "        velocity_dict = dict(velocity_list)\n",
    "        df[f'velocity_{hours}h'] = df.index.map(velocity_dict).fillna(1)\n",
    "    \n",
    "    # Time between transactions\n",
    "    df['time_since_last_txn'] = df.groupby('card1')['TransactionDT'].diff()\n",
    "    df['time_to_next_txn'] = df.groupby('card1')['TransactionDT'].diff(-1).abs()\n",
    "    \n",
    "    # Fill NaN values for time differences\n",
    "    df['time_since_last_txn'].fillna(0, inplace=True)\n",
    "    df['time_to_next_txn'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Cyclical encoding for hour and day\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['TransactionDT_hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['TransactionDT_hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['TransactionDT_day'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['TransactionDT_day'] / 7)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37c2c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Advanced Amount-Based Features\n",
    "def create_amount_features(df):\n",
    "    \"\"\"Sophisticated amount-based features\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic amount transformations\n",
    "    df['TransactionAmt_log'] = np.log1p(df['TransactionAmt'])\n",
    "    df['TransactionAmt_sqrt'] = np.sqrt(df['TransactionAmt'])\n",
    "    df['TransactionAmt_decimal'] = df['TransactionAmt'] - df['TransactionAmt'].astype(int)\n",
    "    df['is_round_amount'] = (df['TransactionAmt_decimal'] == 0).astype(int)\n",
    "    \n",
    "    # Amount patterns\n",
    "    df['amount_digits'] = df['TransactionAmt'].astype(str).str.len()\n",
    "    df['is_even_amount'] = (df['TransactionAmt'] % 2 == 0).astype(int)\n",
    "    df['ends_with_00'] = (df['TransactionAmt'] % 100 == 0).astype(int)\n",
    "    df['ends_with_99'] = (df['TransactionAmt'].astype(str).str.endswith('99')).astype(int)\n",
    "    \n",
    "    # Card-level amount statistics\n",
    "    card_amt_stats = df.groupby('card1')['TransactionAmt'].agg([\n",
    "        'mean', 'std', 'min', 'max', 'count', 'median',\n",
    "        lambda x: np.percentile(x, 25),\n",
    "        lambda x: np.percentile(x, 75)\n",
    "    ]).reset_index()\n",
    "    card_amt_stats.columns = ['card1', 'card_amt_mean', 'card_amt_std', 'card_amt_min', \n",
    "                             'card_amt_max', 'card_amt_count', 'card_amt_median',\n",
    "                             'card_amt_q25', 'card_amt_q75']\n",
    "    \n",
    "    df = df.merge(card_amt_stats, on='card1', how='left')\n",
    "    \n",
    "    # Amount deviation features\n",
    "    df['amt_deviation_from_mean'] = np.abs(df['TransactionAmt'] - df['card_amt_mean']) / (df['card_amt_std'] + 1e-6)\n",
    "    df['amt_rank_within_card'] = df.groupby('card1')['TransactionAmt'].rank(pct=True)\n",
    "    df['is_amt_outlier'] = (df['amt_deviation_from_mean'] > 2).astype(int)\n",
    "    \n",
    "    # Amount vs time patterns\n",
    "    df['amt_per_hour'] = df['TransactionAmt'] / (df['TransactionDT_hour'] + 1)\n",
    "    df['amt_weekend_ratio'] = df['TransactionAmt'] * df['is_weekend']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56e7a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Device Features\n",
    "def create_device_features(df):\n",
    "    \"\"\"Create device and browser-based features\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Device type analysis\n",
    "    if 'DeviceType' in df.columns:\n",
    "        df['is_mobile'] = (df['DeviceType'] == 'mobile').astype(int)\n",
    "        df['is_desktop'] = (df['DeviceType'] == 'desktop').astype(int)\n",
    "    \n",
    "    # Browser analysis\n",
    "    if 'id_31' in df.columns:\n",
    "        df['browser_type'] = df['id_31']\n",
    "        # Common browsers\n",
    "        common_browsers = ['chrome', 'safari', 'firefox', 'edge']\n",
    "        df['is_common_browser'] = df['id_31'].str.lower().isin(common_browsers).astype(int)\n",
    "    \n",
    "    # Screen resolution\n",
    "    if 'id_33' in df.columns:\n",
    "        df['screen_width'] = df['id_33']\n",
    "        df['is_common_resolution'] = df['id_33'].isin([1920, 1366, 1280, 1024]).astype(int)\n",
    "    \n",
    "    # Operating system\n",
    "    if 'id_30' in df.columns:\n",
    "        df['os_type'] = df['id_30']\n",
    "        df['is_windows'] = df['id_30'].str.contains('Windows', na=False).astype(int)\n",
    "        df['is_ios'] = df['id_30'].str.contains('iOS', na=False).astype(int)\n",
    "        df['is_android'] = df['id_30'].str.contains('Android', na=False).astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "642c082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Frequency Encoding with Bayesian Smoothing\n",
    "def frequency_encoding_with_smoothing(df, columns, target='isFraud', alpha=10):\n",
    "    \"\"\"Frequency encoding with Bayesian smoothing to prevent overfitting\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    global_fraud_rate = df[target].mean()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            # Calculate category statistics\n",
    "            stats = df.groupby(col)[target].agg(['count', 'sum']).reset_index()\n",
    "            stats.columns = [col, 'count', 'fraud_count']\n",
    "            \n",
    "            # Bayesian smoothing\n",
    "            stats[f'{col}_fraud_rate_smooth'] = (\n",
    "                (stats['fraud_count'] + alpha * global_fraud_rate) / \n",
    "                (stats['count'] + alpha)\n",
    "            )\n",
    "            \n",
    "            # Frequency encoding\n",
    "            stats[f'{col}_frequency'] = stats['count']\n",
    "            \n",
    "            # Merge back\n",
    "            merge_cols = [col, f'{col}_fraud_rate_smooth', f'{col}_frequency']\n",
    "            df = df.merge(stats[merge_cols], on=col, how='left')\n",
    "            \n",
    "            # Fill missing values with global stats\n",
    "            df[f'{col}_fraud_rate_smooth'].fillna(global_fraud_rate, inplace=True)\n",
    "            df[f'{col}_frequency'].fillna(1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c688c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Advanced Interaction Features\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create meaningful feature interactions\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Amount × Time interactions\n",
    "    df['amt_hour_interaction'] = df['TransactionAmt'] * df['TransactionDT_hour']\n",
    "    df['amt_weekend_interaction'] = df['TransactionAmt'] * df['is_weekend']\n",
    "    df['amt_night_interaction'] = df['TransactionAmt'] * df['is_night']\n",
    "    df['amt_business_interaction'] = df['TransactionAmt'] * df['is_business_hours']\n",
    "    \n",
    "    # Card × Product interactions\n",
    "    if 'ProductCD' in df.columns:\n",
    "        df['card_product_combo'] = df['card1'].astype(str) + '_' + df['ProductCD'].astype(str)\n",
    "    \n",
    "    # Address interactions\n",
    "    if 'addr1' in df.columns and 'addr2' in df.columns:\n",
    "        df['addr_combo'] = df['addr1'].astype(str) + '_' + df['addr2'].astype(str)\n",
    "        df['addr_mismatch'] = (df['addr1'] != df['addr2']).astype(int)\n",
    "    \n",
    "    # Email domain interactions\n",
    "    if 'P_emaildomain' in df.columns and 'R_emaildomain' in df.columns:\n",
    "        df['email_domain_match'] = (df['P_emaildomain'] == df['R_emaildomain']).astype(int)\n",
    "        df['email_domains_combo'] = df['P_emaildomain'].astype(str) + '_' + df['R_emaildomain'].astype(str)\n",
    "    \n",
    "    # Card × Device interactions\n",
    "    if 'id_31' in df.columns:\n",
    "        df['card_browser_combo'] = df['card1'].astype(str) + '_' + df['id_31'].astype(str)\n",
    "    \n",
    "    # Distance features (if geographic data available)\n",
    "    if 'dist1' in df.columns and 'dist2' in df.columns:\n",
    "        df['dist_ratio'] = df['dist1'] / (df['dist2'] + 1e-6)\n",
    "        df['dist_sum'] = df['dist1'] + df['dist2']\n",
    "        df['dist_diff'] = np.abs(df['dist1'] - df['dist2'])\n",
    "    \n",
    "    # Amount percentile interactions\n",
    "    df['amt_rank_hour_interaction'] = df['amt_rank_within_card'] * df['TransactionDT_hour']\n",
    "    df['velocity_amt_interaction'] = df['velocity_1h'] * np.log1p(df['TransactionAmt'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cda9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Network and Anomaly Detection Features (UPDATED)\n",
    "def create_network_anomaly_features(df, is_train=True, train_stats=None):\n",
    "    \"\"\"Create graph-based and anomaly detection features\"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Email domain risk analysis\n",
    "    if 'P_emaildomain' in df.columns:\n",
    "        if is_train:\n",
    "            # Calculate stats from training data\n",
    "            email_stats = df.groupby('P_emaildomain')['isFraud'].agg(['mean', 'count']).reset_index()\n",
    "            email_stats.columns = ['P_emaildomain', 'email_fraud_rate', 'email_frequency']\n",
    "            df = df.merge(email_stats, on='P_emaildomain', how='left')\n",
    "            \n",
    "            # Store stats for test set\n",
    "            if train_stats is not None:\n",
    "                train_stats['email_stats'] = email_stats\n",
    "        else:\n",
    "            # Use pre-calculated stats from training data\n",
    "            if train_stats and 'email_stats' in train_stats:\n",
    "                email_stats = train_stats['email_stats']\n",
    "                df = df.merge(email_stats, on='P_emaildomain', how='left')\n",
    "                # Fill missing with global mean\n",
    "                df['email_fraud_rate'].fillna(email_stats['email_fraud_rate'].mean(), inplace=True)\n",
    "                df['email_frequency'].fillna(1, inplace=True)\n",
    "        \n",
    "        # Email domain categories\n",
    "        free_emails = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com']\n",
    "        df['is_free_email'] = df['P_emaildomain'].isin(free_emails).astype(int)\n",
    "    \n",
    "    # Card family analysis\n",
    "    df['card_family'] = df['card1'] // 1000\n",
    "    if is_train:\n",
    "        card_family_stats = df.groupby('card_family')['isFraud'].agg(['mean', 'count']).reset_index()\n",
    "        card_family_stats.columns = ['card_family', 'card_family_fraud_rate', 'card_family_count']\n",
    "        df = df.merge(card_family_stats, on='card_family', how='left')\n",
    "        \n",
    "        # Store stats for test set\n",
    "        if train_stats is not None:\n",
    "            train_stats['card_family_stats'] = card_family_stats\n",
    "    else:\n",
    "        # Use pre-calculated stats from training data\n",
    "        if train_stats and 'card_family_stats' in train_stats:\n",
    "            card_family_stats = train_stats['card_family_stats']\n",
    "            df = df.merge(card_family_stats, on='card_family', how='left')\n",
    "            # Fill missing with global mean\n",
    "            df['card_family_fraud_rate'].fillna(card_family_stats['card_family_fraud_rate'].mean(), inplace=True)\n",
    "            df['card_family_count'].fillna(1, inplace=True)\n",
    "    \n",
    "    # Isolation Forest for amount anomalies\n",
    "    amount_features = ['TransactionAmt', 'TransactionDT_hour', 'TransactionDT_day']\n",
    "    available_features = [f for f in amount_features if f in df.columns]\n",
    "    \n",
    "    if len(available_features) >= 2:\n",
    "        if is_train:\n",
    "            iso_forest = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)\n",
    "            df['isolation_anomaly'] = iso_forest.fit_predict(df[available_features].fillna(0))\n",
    "            df['isolation_anomaly'] = (df['isolation_anomaly'] == -1).astype(int)\n",
    "            \n",
    "            # Store model for test set\n",
    "            if train_stats is not None:\n",
    "                train_stats['iso_forest'] = iso_forest\n",
    "        else:\n",
    "            # Use pre-fitted model\n",
    "            if train_stats and 'iso_forest' in train_stats:\n",
    "                iso_forest = train_stats['iso_forest']\n",
    "                df['isolation_anomaly'] = iso_forest.predict(df[available_features].fillna(0))\n",
    "                df['isolation_anomaly'] = (df['isolation_anomaly'] == -1).astype(int)\n",
    "    \n",
    "    # Statistical anomalies\n",
    "    df['amt_zscore'] = np.abs(stats.zscore(df['TransactionAmt']))\n",
    "    df['is_amt_extreme'] = (df['amt_zscore'] > 3).astype(int)\n",
    "    \n",
    "    # User behavior consistency\n",
    "    user_behavior_cols = ['TransactionDT_hour', 'is_weekend', 'TransactionAmt']\n",
    "    for col in user_behavior_cols:\n",
    "        if col in df.columns:\n",
    "            user_std = df.groupby('card1')[col].std().reset_index()\n",
    "            user_std.columns = ['card1', f'{col}_user_std']\n",
    "            df = df.merge(user_std, on='card1', how='left')\n",
    "            df[f'{col}_consistency'] = 1 / (df[f'{col}_user_std'] + 1e-6)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c87b06d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering pipeline...\n",
      "Creating temporal features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:171\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/computation/expressions.py:239\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    238\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/computation/expressions.py:70\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     69\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'int'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_processed, test_processed\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Execute the pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m train_engineered, test_engineered = \u001b[43mcomplete_feature_engineering_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcomplete_feature_engineering_pipeline\u001b[39m\u001b[34m(train_data, test_data)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Step 1: Temporal features\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating temporal features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m train_processed = \u001b[43mcreate_temporal_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m test_processed = create_temporal_features(test_data)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Step 2: Amount features\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcreate_temporal_features\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      6\u001b[39m df = df.copy()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Basic time features\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mTransactionDT_hour\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransactionDT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3600\u001b[39;49m) % \u001b[32m24\u001b[39m\n\u001b[32m     10\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mTransactionDT_day\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33mTransactionDT\u001b[39m\u001b[33m'\u001b[39m] / (\u001b[32m3600\u001b[39m * \u001b[32m24\u001b[39m)) % \u001b[32m7\u001b[39m\n\u001b[32m     11\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mTransactionDT_week\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mTransactionDT\u001b[39m\u001b[33m'\u001b[39m] / (\u001b[32m3600\u001b[39m * \u001b[32m24\u001b[39m * \u001b[32m7\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/ops/common.py:81\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     77\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     79\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/arraylike.py:210\u001b[39m, in \u001b[36mOpsMixin.__truediv__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__truediv__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtruediv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/series.py:6112\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6111\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = ops.align_method_SERIES(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m-> \u001b[39m\u001b[32m6112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/base.py:1348\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1345\u001b[39m rvalues = ensure_wrapped_if_datetimelike(rvalues)\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:232\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    228\u001b[39m     _bool_arith_check(op, left, right)\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[32m    231\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:178\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (is_object_dtype(left.dtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(right)):\n\u001b[32m    174\u001b[39m         \u001b[38;5;66;03m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[32m    175\u001b[39m         \u001b[38;5;66;03m#  on the non-missing values)\u001b[39;00m\n\u001b[32m    176\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    177\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         result = \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    180\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Second_Brain/01_Projects/AI/Data_Science_Projects/Credit_Default/venv/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:135\u001b[39m, in \u001b[36m_masked_arith_op\u001b[39m\u001b[34m(x, y, op)\u001b[39m\n\u001b[32m    132\u001b[39m         mask = np.where(y == \u001b[32m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, mask)\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m         result[mask] = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m np.putmask(result, ~mask, np.nan)\n\u001b[32m    138\u001b[39m result = result.reshape(x.shape)  \u001b[38;5;66;03m# 2D compat\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# Complete Feature Engineering Pipeline (UPDATED)\n",
    "def complete_feature_engineering_pipeline(train_data, test_data):\n",
    "    \"\"\"Execute complete feature engineering pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting feature engineering pipeline...\")\n",
    "    \n",
    "    # Initialize train_stats dictionary to store training statistics\n",
    "    train_stats = {}\n",
    "    \n",
    "    # Step 1: Temporal features\n",
    "    print(\"Creating temporal features...\")\n",
    "    train_processed = create_temporal_features(train_data)\n",
    "    test_processed = create_temporal_features(test_data)\n",
    "    \n",
    "    # Step 2: Amount features\n",
    "    print(\"Creating amount features...\")\n",
    "    train_processed = create_amount_features(train_processed)\n",
    "    test_processed = create_amount_features(test_processed)\n",
    "    \n",
    "    # Step 3: Device features\n",
    "    print(\"Creating device features...\")\n",
    "    train_processed = create_device_features(train_processed)\n",
    "    test_processed = create_device_features(test_processed)\n",
    "    \n",
    "    # Step 4: Frequency encoding (only on train to avoid leakage)\n",
    "    print(\"Applying frequency encoding...\")\n",
    "    categorical_cols = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3']\n",
    "    available_cats = [col for col in categorical_cols if col in train_processed.columns]\n",
    "    \n",
    "    train_processed = frequency_encoding_with_smoothing(train_processed, available_cats)\n",
    "    \n",
    "    # Apply same encodings to test set\n",
    "    for col in available_cats:\n",
    "        if f'{col}_fraud_rate_smooth' in train_processed.columns:\n",
    "            # Create encoding maps from train\n",
    "            fraud_rate_map = train_processed.groupby(col)[f'{col}_fraud_rate_smooth'].first().to_dict()\n",
    "            frequency_map = train_processed.groupby(col)[f'{col}_frequency'].first().to_dict()\n",
    "            \n",
    "            # Apply to test\n",
    "            test_processed[f'{col}_fraud_rate_smooth'] = test_processed[col].map(fraud_rate_map).fillna(\n",
    "                train_processed[f'{col}_fraud_rate_smooth'].mean()\n",
    "            )\n",
    "            test_processed[f'{col}_frequency'] = test_processed[col].map(frequency_map).fillna(1)\n",
    "    \n",
    "    # Step 5: Interaction features\n",
    "    print(\"Creating interaction features...\")\n",
    "    train_processed = create_interaction_features(train_processed)\n",
    "    test_processed = create_interaction_features(test_processed)\n",
    "    \n",
    "    # Step 6: Network and anomaly features (UPDATED)\n",
    "    print(\"Creating network and anomaly features...\")\n",
    "    train_processed = create_network_anomaly_features(train_processed, is_train=True, train_stats=train_stats)\n",
    "    test_processed = create_network_anomaly_features(test_processed, is_train=False, train_stats=train_stats)\n",
    "    \n",
    "    print(f\"Feature engineering complete!\")\n",
    "    print(f\"Train shape: {train_processed.shape}\")\n",
    "    print(f\"Test shape: {test_processed.shape}\")\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "# Execute the pipeline\n",
    "train_engineered, test_engineered = complete_feature_engineering_pipeline(train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
